<html>
<head>
    <title>Sebastian Bruch</title>
    <meta charset='UTF-8'>
    <meta content='width=device-width, initial-scale=1' name='viewport'/>
    <meta name='author' content='Sebastian Bruch'>

    <link rel='shortcut icon' href='/favicon.png?v=e' />
    <link href='/css/blog.css' rel='stylesheet'/>

    <script type='text/x-mathjax-config'>
MathJax.Hub.Config({
  jax: ['input/TeX', 'output/HTML-CSS'],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
    extensions: ['color.js']
  },
  messageStyle: 'none',
  'HTML-CSS': { preferredFont: 'TeX', availableFonts: ['STIX','TeX'] }
});
</script>

<script src='//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML' type='text/javascript'></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
</head>
<body>
  <div class="nav">
    <ul class="wrap">
        <li><a href="/">Home</a></li>
        <li><a href="/profile">Profile</a></li>
        <li><a href="/blog">Blog</a></li>
          <li><a rel="me noopener noreferrer" href="https://orcid.org/0000-0002-2469-8242" target="_blank"><svg version="1.1" id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" role="img" width="1em" height="1em" x="0px" y="0px" viewbox="0 0 256 256" style="enable-background:new 0 0 256 256;" xml:space="preserve"><style type="text/css">.st0{fill:#A6CE39;}.st1{fill:#FFFFFF;}</style>
<path class="st0" d="M256,128c0,70.7-57.3,128-128,128C57.3,256,0,198.7,0,128C0,57.3,57.3,0,128,0C198.7,0,256,57.3,256,128z"></path><g><path class="st1" d="M86.3,186.2H70.9V79.1h15.4v48.4V186.2z"></path><path class="st1" d="M108.9,79.1h41.6c39.6,0,57,28.3,57,53.6c0,27.5-21.5,53.6-56.8,53.6h-41.8V79.1z M124.3,172.4h24.5 c34.9,0,42.9-26.5,42.9-39.7c0-21.5-13.7-39.7-43.7-39.7h-23.7V172.4z"></path><path class="st1" d="M88.7,56.8c0,5.5-4.5,10.1-10.1,10.1c-5.6,0-10.1-4.6-10.1-10.1c0-5.6,4.5-10.1,10.1-10.1 C84.2,46.7,88.7,51.3,88.7,56.8z"></path></g></svg> ORCiD</a></li>
          <li><a rel="me noopener noreferrer" href="https://scholar.google.com/citations?user=aAWYKCcAAAAJ&amp;hl=en" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" style="vertical-align:-0.125em;" width="1em" height="1em" preserveaspectratio="xMidYMid meet" viewbox="0 0 24 24"><path d="M5.242 13.769L0 9.5L12 0l12 9.5l-5.242 4.269C17.548 11.249 14.978 9.5 12 9.5c-2.977 0-5.548 1.748-6.758 4.269zM12 10a7 7 0 1 0 0 14a7 7 0 0 0 0-14z" fill="currentColor"></path></svg> Google Scholar</a></li>
          <li><a rel="me noopener noreferrer" href="https://github.com/sbruch" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" style="vertical-align:-0.125em;" width="1em" height="1em" preserveaspectratio="xMidYMid meet" viewbox="0 0 24 24"><g fill="none"><path fill="currentColor" d="M12 2A10 10 0 0 0 2 12c0 4.42 2.87 8.17 6.84 9.5c.5.08.66-.23.66-.5v-1.69c-2.77.6-3.36-1.34-3.36-1.34c-.46-1.16-1.11-1.47-1.11-1.47c-.91-.62.07-.6.07-.6c1 .07 1.53 1.03 1.53 1.03c.87 1.52 2.34 1.07 2.91.83c.09-.65.35-1.09.63-1.34c-2.22-.25-4.55-1.11-4.55-4.92c0-1.11.38-2 1.03-2.71c-.1-.25-.45-1.29.1-2.64c0 0 .84-.27 2.75 1.02c.79-.22 1.65-.33 2.5-.33s1.71.11 2.5.33c1.91-1.29 2.75-1.02 2.75-1.02c.55 1.35.2 2.39.1 2.64c.65.71 1.03 1.6 1.03 2.71c0 3.82-2.34 4.66-4.57 4.91c.36.31.69.92.69 1.85V21c0 .27.16.59.67.5C19.14 20.16 22 16.42 22 12A10 10 0 0 0 12 2"></path></g></svg> Github</a></li>
          <li><a rel="me noopener noreferrer" href="https://www.linkedin.com/in/sebastian-b-8145701b4" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" style="vertical-align:-0.125em;" width="1em" height="1em" preserveaspectratio="xMidYMid meet" viewbox="0 0 24 24"><g fill="none"><path d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-3.368-4-3.113-4 0v5.604h-3v-11h3v1.765c1.396-2.586 7-2.777 7 2.476v6.759z" fill="currentColor"></path></g></svg> LinkedIn</a></li>
          <li><a rel="me noopener noreferrer" href="https://mastodon.social/@bruch" target="_blank"><svg fill="#000000" width="1em" height="1em" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" xml:space="preserve"><path d="M21.327 8.566c0-4.339-2.843-5.61-2.843-5.61-1.433-.658-3.894-.935-6.451-.956h-.063c-2.557.021-5.016.298-6.45.956 0 0-2.843 1.272-2.843 5.61 0 .993-.019 2.181.012 3.441.103 4.243.778 8.425 4.701 9.463 1.809.479 3.362.579 4.612.51 2.268-.126 3.541-.809 3.541-.809l-.075-1.646s-1.621.511-3.441.449c-1.804-.062-3.707-.194-3.999-2.409a4.523 4.523 0 0 1-.04-.621s1.77.433 4.014.536c1.372.063 2.658-.08 3.965-.236 2.506-.299 4.688-1.843 4.962-3.254.434-2.223.398-5.424.398-5.424zm-3.353 5.59h-2.081V9.057c0-1.075-.452-1.62-1.357-1.62-1 0-1.501.647-1.501 1.927v2.791h-2.069V9.364c0-1.28-.501-1.927-1.502-1.927-.905 0-1.357.546-1.357 1.62v5.099H6.026V8.903c0-1.074.273-1.927.823-2.558.566-.631 1.307-.955 2.228-.955 1.065 0 1.872.409 2.405 1.228l.518.869.519-.869c.533-.819 1.34-1.228 2.405-1.228.92 0 1.662.324 2.228.955.549.631.822 1.484.822 2.558v5.253z"></path></svg> Mastodon</a></li>
    </ul>
</div>


  <div id="blog" class="wrap">
    <div class="front-matter">
    <div class="bylines">
    <div class="byline">
      
      <p>Tags: 
      
        <a href="/archive/#Concentration">Concentration</a>
	|
      
        <a href="/archive/#Randomized%20Numerical%20Linear%20Algebra">Randomized Numerical Linear Algebra</a>
	|
      
        <a href="/archive/#Publications">Publications</a>
	
      
      </p>
    </div>
    </div>
    </div>

  <div id="posts" class="section">
    
    <div class="post-row">
      <p class="post-title">
	<a href="/blog/concentration/2024/09/09/concentration-martingale-difference-sequences/">
	  
	  Concentration of Martingale Difference Sequences
	  
	</a>
      </p>
      <p class="post-date">
	09 September 2024
      </p>
    </div>
    <p class="post-subtitle">
      
      Sub-exponentialness is such an important property, as we saw in earlier posts. We learnt, for example, that sums of independent sub-exponential random variables are themselves sub-exponential random variables, allowing us to extend concentration results to sums. In this post, I'll review another important result that allows us to apply the same results (such as Hoeffding's inequality) to <i>functions</i> of independent random variables, under certain conditions.
      
    </p>
    <span class="hidden">1</span>
  
    <div class="post-row">
      <p class="post-title">
	<a href="/blog/randomized%20numerical%20linear%20algebra/2024/08/25/oblivious-subspace-embedding/">
	  
	  Oblivious Subspace Embedding
	  
	</a>
      </p>
      <p class="post-date">
	25 August 2024
      </p>
    </div>
    <p class="post-subtitle">
      
      In an earlier post, we saw how <a href="/blog/randomized%20numerical%20linear%20algebra/2024/08/12/jl-transform">the Johnson-Lindenstrauss Transform</a> can reduce the dimensionality of a set of m points while preserving their pairwise distances. That's neat, but what if we need to preserve the pairwise distance between all (infinitely many) points in an entire subspace? This post introduces the concept of subspace embedding, which answers that question.
      
    </p>
    <span class="hidden">2</span>
  
    <div class="post-row">
      <p class="post-title">
	<a href="/blog/randomized%20numerical%20linear%20algebra/2024/08/25/epsilon-nets/">
	  
	  Epsilon Nets
	  
	</a>
      </p>
      <p class="post-date">
	25 August 2024
      </p>
    </div>
    <p class="post-subtitle">
      
      A common technique in arguments in sketching is to appeal to the concept of an Epsilon Net. This article explains what that is and why it always exists.
      
    </p>
    <span class="hidden">3</span>
  
    <div class="post-row">
      <p class="post-title">
	<a href="/blog/concentration/2024/08/23/subexponential-rvs/">
	  
	  Concentration of Sub-Exponential Random Variables
	  
	</a>
      </p>
      <p class="post-date">
	23 August 2024
      </p>
    </div>
    <p class="post-subtitle">
      
      This post reviews the definition of sub-exponential random variables and discusses useful tail bounds (Bernstein) for this family.
      
    </p>
    <span class="hidden">4</span>
  
    <div class="post-row">
      <p class="post-title">
	<a href="/blog/concentration/2024/08/15/subgaussian-rvs/">
	  
	  Concentration of Sub-Gaussian Random Variables
	  
	</a>
      </p>
      <p class="post-date">
	15 August 2024
      </p>
    </div>
    <p class="post-subtitle">
      
      This post reviews basic tail bounds (Markov, Chebyshev, Chernoff, Hoeffding) and explains the concept of sub-Gaussian random variables.
      
    </p>
    <span class="hidden">5</span>
  
    <div class="post-row">
      <p class="post-title">
	<a href="/blog/randomized%20numerical%20linear%20algebra/2024/08/12/jl-transform/">
	  
	  Johnson-Lindenstrauss Transform
	  
	</a>
      </p>
      <p class="post-date">
	12 August 2024
      </p>
    </div>
    <p class="post-subtitle">
      
      The JL Lemma offers a simple and elegant machinery to reduce the dimensionality of a fixed set of high-dimensional points while approximately preserving their pairwise Euclidean distance. This post explains the concept and describes simple randomized constructions of such a transformation.
      
    </p>
    <span class="hidden">6</span>
  
    <div class="post-row">
      <p class="post-title">
	<a href="/blog/randomized%20numerical%20linear%20algebra/2024/07/29/rnla-schatten-norm/">
	  
	  Schatten Norm Approximation
	  
	</a>
      </p>
      <p class="post-date">
	29 July 2024
      </p>
    </div>
    <p class="post-subtitle">
      
      Randomized Numerical Linear Algebra is a gorgeous field that embraces chaos and whose results often seem wonderfully magical. My very first post on this subject explains a beautiful algorithm to approximate Schatten norms of a matrix.
      
    </p>
    <span class="hidden">7</span>
  
    <div class="post-row">
      <p class="post-title">
	<a href="/blog/publications/2024/07/11/seismic/">
	  
	  Efficient Inverted Indexes for Approximate Retrieval over Learned Sparse Representations
	  
	</a>
      </p>
      <p class="post-date">
	11 July 2024
      </p>
    </div>
    <p class="post-subtitle">
      
      This article describes a novel retrieval algorithm developed for collections of sparse vectors. This work, which is <a href="https://dl.acm.org/doi/abs/10.1145/3626772.3657769" target="_blank" rel="noopener noreferrer">published in the proceedings of SIGIR'24</a>, is the culmination of a joint collaboration with my brilliant colleagues Rossano Venturini (of the University of Pisa), Cosimo Rulli and Franco Maria Nardini (both of the Italian National Research Council, ISTI-CNR). Our proposed algorithm marks a seismic shift in retrieval over learnt sparse embeddings of text collections in accuracy and speed. We are all equally proud of this paper! 
      
    </p>
    <span class="hidden">8</span>
  
    <div class="post-row">
      <p class="post-title">
	<a href="/blog/publications/2024/05/01/foundations-of-vector-retrieval/">
	  
	  Foundations of Vector Retrieval
	  
	</a>
      </p>
      <p class="post-date">
	01 May 2024
      </p>
    </div>
    <p class="post-subtitle">
      
      A monograph devoted to the theoretical foundations of nearest neighbor search, with a discussion of key data structures and algorithms from this vast literature.
      
    </p>
    <span class="hidden">9</span>
  
</div>

</div></body>
</html>
